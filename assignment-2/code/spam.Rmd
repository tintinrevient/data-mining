---
title: 'Assignment 2: Classification for the Detection of Opinion Spam'
author: "Anouk van der Lee (6620590), Shu Zhao (6833519), Fleur Petit (5583837)"
date: "`r format(Sys.time(), '%d, %B, %Y')`"
output: 
  pdf_document:
    toc : true
---

```{r library, include=FALSE}
library(tidyverse)
library(knitr)
library(tm)
library(entropy)
library(SnowballC)
library(glmnet)
library(rpart)
library(rpart.plot)
library(randomForest)
```

```{r basic_functions, include=FALSE}
# train by multi-normial bayes
train.mnb <- function (dtm,labels) {
  call <- match.call()
  V <- ncol(dtm)
  N <- nrow(dtm)
  prior <- table(labels)/N
  labelnames <- names(prior)
  nclass <- length(prior)
  cond.probs <- matrix(nrow=V,ncol=nclass)
  dimnames(cond.probs)[[1]] <- dimnames(dtm)[[2]]
  dimnames(cond.probs)[[2]] <- labelnames
  index <- list(length=nclass)
  for(j in 1:nclass){
    index[[j]] <- c(1:N)[labels == labelnames[j]]
  }
  
  for(i in 1:V){
    for(j in 1:nclass){
      cond.probs[i,j] <- (sum(dtm[index[[j]],i])+1)/(sum(dtm[index[[j]],])+V)
    }
  }
  list(call=call,prior=prior,cond.probs=cond.probs)    
}

# predict by multi-normial bayes
predict.mnb <- function (model,dtm) {
  classlabels <- dimnames(model$cond.probs)[[2]]
  logprobs <- dtm %*% log(model$cond.probs)
  N <- nrow(dtm)
  nclass <- ncol(model$cond.probs)
  logprobs <- logprobs+matrix(nrow=N,ncol=nclass,log(model$prior),byrow=T)
  classlabels[max.col(logprobs)]
}

# return top-n high mutual-information terms' index
calculate.topn <- function(dtm, topn) {
  # Convert document term matrix to binary (term present/absent)
  dtm.bin <- as.matrix(dtm) > 0
  
  # Compute mutual information of each term with class label
  mi <- apply(as.matrix(dtm.bin),
              2, 
              function(x,y){ mi.plugin(table(x,y)/length(y), unit="log2")},
              labels[index.train])
  
  # Sort the indices from high to low mutual information
  mi.order <- order(mi, decreasing = TRUE)
  
  # Show top-n mutual-information terms
  mi[mi.order[topn]]
  
  # Return top-n mutual-information terms' index
  return(mi.order[topn])
}

# performance metrics
performance <- function(confusion_matrix){
  tn <- confusion_matrix[1,1]
  fn <- confusion_matrix[2,1]
  n_pred <- tn+fn
  
  fp <- confusion_matrix[1,2]
  tp <- confusion_matrix[2,2]
  p_pred <- fp+tp
  
  n <- tn+fp
  p <- fn+tp
  
  performance_metrics <- tibble(metric = c("recall",
                                "miss-rate", # 1 - recall
                                "fall-out", # 1 - selectivity
                                "selectivity",
                                "prevalence",
                                "precision",
                                "false omission rate", # 1 - neg_predictive_value
                                "pos likelihood ratio",
                                "neg likelihood ratio",
                                "accuracy",
                                "false discovery rate", # 1 - precision
                                "neg predictive value",
                                "diagnostic odds ratio",
                                "F1"),
                      value = c(tp/p, # recall
                               fn/p, # 1 - recall
                               fp/n, # 1 - selectivity
                               tn/n, # selectivity
                               p/(n+p), # prevalence
                               tp/p_pred, # precision
                               fn/n_pred, # 1 - neg_predictive_value
                               (tp/p)/(fp/n), # positive likelihood ratio
                               (fn/p)/(tn/n), # negative likelihood ratio
                               (tp+tn)/(n+p), # accuracy
                               fp/p_pred, # 1 - precision
                               tn/n_pred, # negative predictive value
                               ((tp/p)/(fp/n))/((fn/p)/(tn/n)), # diagnostic odds ratio
                               2*(tp/p_pred)*(tp/p)/((tp/p_pred) + (tp/p))) # F1
  )
  
  performance_metrics
}
```

# Dataset for training, validation and test

## Dataset split

```{r load_reviews, include=FALSE}
# Read in the data using UTF-8 encoding
reviews.false <- VCorpus(DirSource(file.path("data", "op_spam_v1.4", "negative_polarity", "deceptive_from_MTurk"), 
                                       encoding = "UTF-8",
                                       recursive = TRUE))
reviews.true <- VCorpus(DirSource(file.path("data", "op_spam_v1.4", "negative_polarity", "truthful_from_Web"), 
                                      encoding = "UTF-8",
                                      recursive = TRUE))

# Join true and false reviews into a single Corpus
reviews.all <- c(reviews.false, reviews.true)

# Create label vector
# 1 = deceptive negative, 0 = truthful negative (as deceptive negative is what we are interested in!!!)
labels <- c(rep(1, length(reviews.false)), rep(0, length(reviews.true)))
```

```{r preprocess_corpus, include=FALSE}
# Preprocess the corpus
reviews.all <- reviews.all %>%
  # Remove punctuation marks (commaâ€™s, etc.)
  tm_map(removePunctuation) %>% 
  # Make all letters lower case
  tm_map(content_transformer(tolower)) %>% 
  # Remove English stopwords
  tm_map(removeWords, stopwords("english")) %>% 
  # Remove numbers
  tm_map(removeNumbers) %>% 
  # Remove excess whitespace
  tm_map(stripWhitespace)
```

```{r train_and_test_dtm}
# 1 fold = 80 samples
# Fold 1-4 for training = 1:320 from true + 1:320 from false
index.train <- c(c(1:320), 400+c(1:320))

# Training document-term matrix
train.dtm <- DocumentTermMatrix(reviews.all[index.train])
# Test document-term matrix
test.dtm <- DocumentTermMatrix(reviews.all[-index.train], 
                               list(dictionary = dimnames(train.dtm)[[2]]))

# Training document-term matrix for bigrams
BigramTokenizer <- function (x) {
  unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
    
train.dtm.bigram <- DocumentTermMatrix(reviews.all[index.train],
                                       control = list(tokenize = BigramTokenizer))

# Test document-term matrix for bigrams
test.dtm.bigram <- DocumentTermMatrix(reviews.all[-index.train], 
                                      list(dictionary = dimnames(train.dtm.bigram)[[2]]))
```

## Feature selection

```{r frequency_based_feature_selection}
# Remove terms that occur in less than 5% of the documents
# Training document-term matrix
train.dtm.freq <- removeSparseTerms(train.dtm, 0.95)
# Test document-term matrix
test.dtm.freq <- DocumentTermMatrix(reviews.all[-index.train], 
                                    list(dictionary = dimnames(train.dtm.freq)[[2]]))

# Training document-term matrix for bigrams
train.dtm.bigram.freq <- removeSparseTerms(train.dtm.bigram, 0.99)
# Test document-term matrix for bigrams
test.dtm.bigram.freq <- DocumentTermMatrix(reviews.all[-index.train], 
                                           list(dictionary = dimnames(train.dtm.bigram.freq)[[2]]))

# Combine unigrams and bigrams
# Training document-term matrix for unigrams and bigrams
train.dtm.bigram.freq <- cbind(train.dtm.freq, train.dtm.bigram.freq)
# Test document-term matrix for unigrams and bigrams
test.dtm.bigram.freq <- cbind(test.dtm.freq, test.dtm.bigram.freq)
```

```{r mutual_info_feature_selection}

# Select top-n mutual-information terms from total vocabularies
index.top300 <- calculate.topn(train.dtm, topn = c(1:300))
train.dtm.top300 <- train.dtm[, index.top300]
test.dtm.top300 <- test.dtm[, index.top300]

# Select top-n mutual-information terms from frequent terms
index.freq.top200 <- calculate.topn(train.dtm.freq, topn = c(1:200))
train.dtm.freq.top200 <- train.dtm[, index.freq.top200]
test.dtm.freq.top200 <- test.dtm[, index.freq.top200]
```

# Classifiers

## Multinomial naive Bayes

```{r classifier_mnb}
# 4-fold cross validation
reviews.mnb.pred <- c()
reviews.mnb.actual <- c()

# hyper-parameter: feature selection
# the best option: only mutual information to select top-300 terms out of 6900 total terms
train.dtm.mnb <- train.dtm.top300

for(i in 1:4) {
  # Validation fold
  val.start <- (i - 1) * 80 + 1
  val.end <- val.start + 80 - 1
  val.range <- c(c(val.start:val.end), 320+c(val.start:val.end))
  
  # Training fold
  train.range <- c(c(1:320)[-val.range], 320+c(1:320)[-val.range])
  
  # Train the model with priors and conditional probabilities
  reviews.mnb <- train.mnb(as.matrix(train.dtm.mnb)[train.range,], labels[index.train][train.range])
  # Make predictions
  reviews.mnb.pred <- c(reviews.mnb.pred, predict.mnb(reviews.mnb, as.matrix(train.dtm.mnb)[val.range,]))
  reviews.mnb.actual <- c(reviews.mnb.actual, labels[index.train][val.range])
}

# Confusion matrix
conf.mat.mnb <- table(reviews.mnb.actual, reviews.mnb.pred, dnn = c("actual", "predicted"))
perf.mnb <-performance(conf.mat.mnb) 

perf.mnb %>% kable()
```

```{r classifier_mnb_test_performance, include=FALSE}
# Final training set and test set
train.dtm.mnb <- train.dtm.top300
test.dtm.mnb <- test.dtm.top300

# Train the model with priors and conditional probabilities
reviews.mnb.test <- train.mnb(as.matrix(train.dtm.mnb), labels[index.train])
# Make predictions
reviews.mnb.test.pred <- predict.mnb(reviews.mnb.test, as.matrix(test.dtm.mnb))

# Confusion matrix
conf.mat.mnb.test <- table(labels[-index.train], reviews.mnb.test.pred, dnn = c("actual", "predicted"))

perf.mnb.test <-performance(conf.mat.mnb.test) 
perf.mnb.test %>% kable()
```

```{r classifier_mnb_with_bigram}
# 4-fold cross validation
reviews.mnb.bigram.pred <- c()
reviews.mnb.bigram.actual <- c()

# hyper-parameter: feature selection
# the best option: only mutual information to select top-300 terms out of 6900 total terms
train.dtm.mnb.bigram <- train.dtm.bigram.freq

for(i in 1:4) {
  # Validation fold
  val.start <- (i - 1) * 80 + 1
  val.end <- val.start + 80 - 1
  val.range <- c(c(val.start:val.end), 320+c(val.start:val.end))
  
  # Training fold
  train.range <- c(c(1:320)[-val.range], 320+c(1:320)[-val.range])
  
  # Train the model with priors and conditional probabilities
  reviews.mnb <- train.mnb(as.matrix(train.dtm.mnb.bigram)[train.range,], labels[index.train][train.range])
  # Make predictions
  reviews.mnb.bigram.pred <- c(reviews.mnb.bigram.pred, 
                               predict.mnb(reviews.mnb, as.matrix(train.dtm.mnb.bigram)[val.range,]))
  reviews.mnb.bigram.actual <- c(reviews.mnb.bigram.actual, labels[index.train][val.range])
}

# Confusion matrix
conf.mat.mnb.bigram <- table(reviews.mnb.bigram.actual, reviews.mnb.bigram.pred, 
                             dnn = c("actual", "predicted"))
perf.mnb.bigram <-performance(conf.mat.mnb.bigram) 

perf.mnb.bigram %>% kable()
```

## Regularized logistic regression

```{r logistic_regression}
# hyper-parameter: feature selection
# the best option: ???
train.dtm.glmet <- train.dtm.freq
test.dtm.glmet <- test.dtm.freq

# Logistic regression with lasso penalty
reviews.glmnet <- cv.glmnet(as.matrix(train.dtm.glmet), labels[index.train], 
                            family="binomial", type.measure="class")
# Cross-validation on lambda
plot(reviews.glmnet)

# coef(reviews.glmnet, s="lambda.1se")

# Make predictions on the test set
reviews.logreg.pred <- predict(reviews.glmnet, newx=as.matrix(test.dtm.glmet), 
                               s="lambda.1se", type="class")

# Confusion matrix
conf.mat.logreg <- table(labels[-index.train], reviews.logreg.pred, dnn = c("actual", "predicted"))

perf.logreg <-performance(conf.mat.logreg) 
perf.logreg %>% kable()
```

```{r logistic_regression_with_bigram}
# hyper-parameter: feature selection
# the best option: ???
train.dtm.glmet.bigram <- train.dtm.bigram.freq
test.dtm.glmet.bigram <- test.dtm.bigram.freq

# Logistic regression with lasso penalty
reviews.glmnet.bigram <- cv.glmnet(as.matrix(train.dtm.glmet.bigram), labels[index.train], 
                                   family="binomial", type.measure="class")
# Cross-validation on lambda
plot(reviews.glmnet.bigram)

# coef(reviews.glmnet.bigram, s="lambda.1se")

# Make predictions on the test set
reviews.logreg.bigram.pred <- predict(reviews.glmnet.bigram, newx=as.matrix(test.dtm.glmet.bigram), 
                                      s="lambda.1se", type="class")

# Confusion matrix
conf.mat.logreg.bigram <- table(labels[-index.train], reviews.logreg.bigram.pred, 
                                dnn = c("actual", "predicted"))

perf.logreg.bigram <-performance(conf.mat.logreg.bigram) 
perf.logreg.bigram %>% kable()
```

## Classification trees

```{r classifier_rpart}
# hyper-parameter: feature selection
# the best option: ???
train.dtm.rpart <- train.dtm.freq
test.dtm.rpart <- test.dtm.freq

# Grow the tree
reviews.rpart <- rpart(label ~ ., 
                       data = data.frame(as.matrix(train.dtm.rpart), label = labels[index.train]), 
                       cp = 0,
                       method = "class")

# Plot cv-error of pruning sequence
plotcp(reviews.rpart)
# Tree with lowest cv error
reviews.rpart.pruned <- prune(reviews.rpart, cp=0.014)
# Plot the tree
rpart.plot(reviews.rpart.pruned)

# Make predictions on the test set
reviews.rpart.pred <- predict(reviews.rpart.pruned,
                              newdata = data.frame(as.matrix(test.dtm.rpart)), 
                              type = "class")

# Confusion matrix
conf.mat.rpart <- table(labels[-index.train], reviews.rpart.pred, dnn = c("actual", "predicted"))

perf.rpart <-performance(conf.mat.rpart) 
perf.rpart %>% kable()
```

```{r classifier_rpart_bigram}
# hyper-parameter: feature selection
# the best option: ???
train.dtm.rpart.bigram <- train.dtm.bigram.freq
test.dtm.rpart.bigram <- test.dtm.bigram.freq

# Grow the tree
reviews.rpart.bigram <- rpart(label ~ ., 
                              data = data.frame(as.matrix(train.dtm.rpart.bigram), 
                                                label = labels[index.train]), 
                              cp = 0, method = "class")

# Plot cv-error of pruning sequence
plotcp(reviews.rpart.bigram)
# Tree with lowest cv error
reviews.rpart.bigram.pruned <- prune(reviews.rpart.bigram, cp=0.014)
# Plot the tree
rpart.plot(reviews.rpart.bigram.pruned)

# Make predictions on the test set
reviews.rpart.bigram.pred <- predict(reviews.rpart.bigram.pruned,
                                     newdata = data.frame(as.matrix(test.dtm.rpart.bigram)), 
                                     type = "class")

# Confusion matrix
conf.mat.rpart.bigram <- table(labels[-index.train], reviews.rpart.bigram.pred, 
                               dnn = c("actual", "predicted"))

perf.rpart.bigram <-performance(conf.mat.rpart.bigram) 
perf.rpart.bigram %>% kable()
```

## Random forests

```{r classifier_rf}
# hyper-parameter: feature selection
# the best option: ???
train.dtm.rf <- train.dtm.freq
test.dtm.rf <- test.dtm.freq

# Train random forest with default settings: 500 trees and mtry = 17
reviews.rf <- randomForest(as.factor(label) ~ ., 
                           data = data.frame(as.matrix(train.dtm.rf), label = labels[index.train]))
# Make predictions
reviews.rf.pred <- predict(reviews.rf, 
                           newdata = data.frame(as.matrix(test.dtm.rf), label = labels[-index.train]))

# Confusion matrix
conf.mat.rf <- table(labels[-index.train], reviews.rf.pred, dnn = c("actual", "predicted"))

perf.rf <-performance(conf.mat.rf) 
perf.rf %>% kable()
```

```{r classifier_rf_bigram}
# hyper-parameter: feature selection
# the best option: ???
train.dtm.rf.bigram <- train.dtm.bigram.freq
test.dtm.rf.bigram <- test.dtm.bigram.freq

# Train random forest with default settings: 500 trees and mtry = 17
reviews.rf.bigram <- randomForest(as.factor(label) ~ ., 
                                  data = data.frame(as.matrix(train.dtm.rf.bigram), 
                                                    label = labels[index.train]))
# Make predictions
reviews.rf.bigram.pred <- predict(reviews.rf.bigram, 
                                  newdata = data.frame(as.matrix(test.dtm.rf.bigram), 
                                                       label = labels[-index.train]))

# Confusion matrix
conf.mat.rf.bigram <- table(labels[-index.train], reviews.rf.bigram.pred, dnn = c("actual", "predicted"))

perf.rf.bigram <-performance(conf.mat.rf.bigram) 
perf.rf.bigram %>% kable()
```

# Hyper-parameters

# Questions

1. For a single classification tree, the impurity reduction is not equal to mutual information?
2. the words found in feature selection (e.g. frequqency or mutual information) = the words found in classification tree by impurity reduction = the words found in logistic regression by coef(s="lambda.1se") (e.g. as.matrix(coef(reviews.glmnet, s="lambda.1se"))[,1]['chicago'])? 
3. sum(as.matrix(coef(reviews.glmnet, s="lambda.1se")) != 0) is 52, so top50 is better?

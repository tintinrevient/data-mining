---
title: 'Assignment 2: Classification for the Detection of Opinion Spam'
author: "Anouk van der Lee (6620590), Shu Zhao (6833519), Fleur Petit (5583837)"
date: "`r format(Sys.time(), '%d, %B, %Y')`"
output: 
  pdf_document:
    toc : true
---

```{r setup, include=FALSE}
library(tidyverse)
library(tm)
library(knitr)
library(kableExtra)
library(glmnet)
library(randomForest)
library(SnowballC)
library(broom)
library(NLP)
library(coefplot)
library(DT)
library(ggfortify)
library(cowplot)

knitr::opts_chunk$set(echo = TRUE)


theme_set(theme_bw())

```

# TO DO

- Comparison of results

# 0. Pre-processing

## Load reviews

```{r prepare_df}


df <- NULL

file_names <-
  list.files(path = "negative_polarity", recursive = T)

for (name in file_names) {
  df <- bind_rows(df,
    list(fold = as.numeric(str_extract(name, pattern = regex('\\d'))),
      hotel = str_extract(name, pattern = regex('[a-z]*_\\d+')),
      review = read_file(str_c("negative_polarity/", name)),
      label = as.numeric(str_detect(name, "deceptive")) # 1 for deceptive and 0 for truthful
    )
  )
}

df <- df %>%
  mutate(id = as.numeric(str_extract(hotel, pattern = regex('\\d+'))),
         hotel = str_extract(hotel, pattern = regex('[a-z]*'))
         ) %>%
  arrange(fold)

head(df, 2) %>%
  kable()

```

## Prepare corpus

```{r prepare_corpus}

corpus <- VCorpus(VectorSource(df[["review"]])) %>%
  tm_map(., content_transformer(tolower)) %>% # no capital letters
  tm_map(., stripWhitespace) %>% # remove extra white space
  tm_map(., removeWords, stopwords("english")) %>% # remove stopwords
  tm_map(., stemDocument) # stem words

```

## Get word frequency matrix

### Unigrams

```{r unigram}

length(corpus)

dtm <- DocumentTermMatrix(corpus)  %>%
  removeSparseTerms(., sparse = 0.95) # those terms from x are removed which have at least a 70
# percent of empty (i.e., terms occurring 0 times in a document) elements. I.e., the resulting
# matrix contains only terms with a sparse factor of less than 70 percent

dim(dtm)

inspect(dtm)


```

### Bigrams

```{r bigram}

BigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)

dtm2 <- DocumentTermMatrix(corpus, control = list(tokenize = BigramTokenizer))  %>%
  removeSparseTerms(., sparse = 0.99)

dim(dtm2)

inspect(dtm2)

```

## Vector with labels

```{r labels}

labels <- as.factor(df[["label"]])

labels[c(1:10, 790:800)]

```

## Random split in train and test data

### Unigrams

```{r train_test_split_uni}

# training data does not have to be sampled at random
# set.seed(123)

#train_ind <- sample(1:nrow(df), size = nrow(df)*0.75)

train_ind <- 1:640

train_dtm <- as.matrix(dtm)[train_ind,]
train_labels <- labels[train_ind]

test_dtm <- as.matrix(dtm)[-train_ind,]
test_labels <- labels[-train_ind]

df[train_ind, "set"] <- "train"
df[-train_ind, "set"] <- "test"

ggplot(df, aes(set, fill = labels)) +
  geom_bar(position = "fill")



```

```{r}

# Info per hotel, not so relevant

# ggplot(df, aes(hotel, fill = as.factor(label))) +
#   geom_bar() +
#   theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
# 
# ggplot(df, aes(hotel, fill = set)) +
#   geom_bar() +
#   theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
# 
# ggplot(df[train_ind,], aes(hotel, fill = as.factor(label))) +
#   geom_bar(position = "fill") +
#   theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

### Bigrams

```{r train_test_split_bi}

train_dtm2 <- as.matrix(dtm2)[train_ind,]

test_dtm2 <- as.matrix(dtm2)[-train_ind,]

```

# 1. Multinomial naive Bayes (generative linear classifier)

## Function for multinomial Bayes classifier

```{r mnb}

# Function for multinomial naive Bayes

train.mnb <- function (dtm, labels) {
  call <- match.call()
  V <- ncol(dtm)
  N <- nrow(dtm)
  prior <- table(labels) / N
  labelnames <- names(prior)
  nclass <- length(prior)
  cond.probs <- matrix(nrow = V, ncol = nclass)
  dimnames(cond.probs)[[1]] <- dimnames(dtm)[[2]]
  dimnames(cond.probs)[[2]] <- labelnames
  index <- list(length = nclass)
  for (j in 1:nclass) {
    index[[j]] <- c(1:N)[labels == labelnames[j]]
  }
  
  for (i in 1:V) {
    for (j in 1:nclass) {
      cond.probs[i, j] <-
        (sum(dtm[index[[j]], i]) + 1) / (sum(dtm[index[[j]], ]) + V)
    }
  }
  list(call = call,
       prior = prior,
       cond.probs = cond.probs)
}

predict.mnb <- function (model, dtm) {
    classlabels <- dimnames(model$cond.probs)[[2]]
    logprobs <- dtm %*% log(model$cond.probs)
    N <- nrow(dtm)
    nclass <- ncol(model$cond.probs)
    logprobs <-
      logprobs + matrix(nrow = N,
                        ncol = nclass,
                        log(model$prior),
                        byrow = T)
    classlabels[max.col(logprobs)]
  }


```

## Train

```{r train_mnb}

mnb_model <- train.mnb(train_dtm, train_labels)

mnb_model$cond.prob %>% head(10) %>% kable()

```

## Predict

```{r predict_mnb}

predicted_mnb <- predict.mnb(mnb_model, test_dtm)

```

## Confusion matrix

```{r confusion_mnb}

test_preds_mnb <- tibble(review_type = ifelse(test_labels==1,"deceptive","truthful"),
                     prediction = ifelse(predicted_mnb==1, "deceptive", "truthful"),
                     correct = ifelse(review_type == prediction, "1", "0")) %>%
  rowid_to_column("document") 

ggplot(test_preds_mnb, aes(document, prediction, colour = correct)) +
  geom_point(shape = 3, size = 4)# +
  #facet_grid(~review_type, scales = "free")

ggplot(test_preds_mnb, aes(prediction, fill = correct)) +
  geom_bar(position = "fill")

ggplot(test_preds_mnb, aes(review_type, fill = correct)) +
  geom_bar(position = "fill")

conf_mnb <- table(test_labels, predicted_mnb)

conf_mnb

```

## Performance metrics

```{r performance_mnb}

performance <- function(confusion_matrix){
  tn <- confusion_matrix[1,1]
  fn <- confusion_matrix[2,1]
  n_pred <- tn+fn
  
  fp <- confusion_matrix[1,2]
  tp <- confusion_matrix[2,2]
  p_pred <- fp+tp
  
  n <- tn+fp
  p <- fn+tp
  
  performance_metrics <- tibble(metric = c("recall",
                                "miss-rate", # 1 - recall
                                "fall-out", # 1 - selectivity
                                "selectivity",
                                "prevalence",
                                "precision",
                                "false omission rate", # 1 - neg_predictive_value
                                "pos likelihood ratio",
                                "neg likelihood ratio",
                                "accuracy",
                                "false discovery rate", # 1 - precision
                                "neg predictive value",
                                "diagnostic odds ratio",
                                "F1"),
                      value = c(tp/p, # recall
                               fn/p, # 1 - recall
                               fp/n, # 1 - selectivity
                               tn/n, # selectivity
                               p/(n+p), # prevalence
                               tp/p_pred, # precision
                               fn/n_pred, # 1 - neg_predictive_value
                               (tp/p)/(fp/n), # positive likelihood ratio
                               (fn/p)/(tn/n), # negative likelihood ratio
                               (tp+tn)/(n+p), # accuracy
                               fp/p_pred, # 1 - precision
                               tn/n_pred, # negative predictive value
                               ((tp/p)/(fp/n))/((fn/p)/(tn/n)), # diagnostic odds ratio
                               2*(tp/p_pred)*(tp/p)/((tp/p_pred) + (tp/p))) # F1
  )
  
  performance_metrics
}

perf_mnb <-performance(conf_mnb) 

perf_mnb %>% kable()


```

# 2. Regularised logistic regression (discriminative linear classifier)

A logistic regeression model uses the information from the document term matrix to predict the class (either '1' for deceptive or '0' for truthful). For each  

## Unigram

### Choose lambda

```{r cv_lasso}

# Use LASSO (penalising for number of parameters)
# Determine lambda by means of cross validation

cv_lasso <- cv.glmnet(train_dtm, train_labels, alpha = 1, family = "binomial", type.measure="class")

lambda_uni <- plot(cv_lasso)

coefs <- as.data.frame(as.matrix(coef(cv_lasso, s="lambda.1se"))) %>%
  rownames_to_column(var = "term") %>%
  rename(coefficient = `1`) %>%
  filter(coefficient != 0)

top10coef <- coefs %>% 
  filter(term != "(Intercept)") %>% 
  arrange(desc(abs(coefficient))) %>% 
  head(10) %>%
  mutate(probability = exp(coefficient)/(exp(coefficient)+1))

top10coef %>%
  kable()


```

### Train

```{r train_glm, fig.height=10}

model_glm <- glmnet(train_dtm, train_labels, alpha = 1, family = "binomial", 
                    lambda = cv_lasso$lambda.1se, type.measure="class") # I choose the largest lambda within 1se from the smallest lambda

coefplot(model_glm, lambda=cv_lasso$lambda.1se, sort='magnitude', cex=100)


```

### Predict

```{r predict_glm, fig.width = 10}

#test_dtm_glm <- model.matrix(test_labels ~ test_dtm)[,-1]

predicted_glm <- predict(model_glm, 
                             newx = test_dtm, 
                             type = "class", # Type "response" gives the fitted probabilities for "binomial"
                             s= cv_lasso$lambda.1se)


```

```{r}

# Information about hotels and wrong predictions

# wrong_reviews <- df[-train_ind,] %>%
#   filter(test_preds[["correct"]]==0)
# 
# wrong_reviews <- as.data.frame(test_dtm[test_preds[["correct"]]==0,]) %>%
#   gather(key = "word", value = "count") %>%
#   group_by(word) %>%
#   summarise(n = sum(count))
# 
# per_hotel <- df[-train_ind,] %>%
#   mutate(correct = test_preds[["correct"]]==1)
# 
# ggplot(per_hotel, aes(hotel, fill = correct)) +
#   geom_bar() + 
#   theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

### Confusion matrix

```{r confusion_glm}

test_preds <- tibble(review_type = ifelse(test_labels==1,"deceptive","truthful"),
                     prediction = ifelse(predicted_glm==1, "deceptive", "truthful"),
                     correct = ifelse(review_type == prediction, "1", "0")) %>%
  rowid_to_column("document") 

ggplot(test_preds, aes(document, prediction, colour = correct)) +
  geom_point(shape = 3, size = 4)# +
  #facet_grid(~review_type, scales = "free")

prec <- ggplot(test_preds, aes(prediction, fill = correct)) +
  geom_bar(position = "fill")

recall <- ggplot(test_preds, aes(review_type, fill = correct)) +
  geom_bar(position = "fill")

conf_glm <- table(test_labels, predicted_glm)

conf_glm

```


### Performance metrics

```{r performance_glm}

perf_glm <- performance(conf_glm)

perf_glm %>% kable()

```

## Bigram

### Choose lambda

```{r cv_lasso2}

# Use LASSO (penalising for number of parameters)
# Determine lambda by means of cross validation

cv_lasso2 <- cv.glmnet(train_dtm2, train_labels, alpha = 1, family = "binomial", type.measure = "class")

lambda_bi <-plot(cv_lasso2)

coefs2 <- as.data.frame(as.matrix(coef(cv_lasso2, s="lambda.1se"))) %>%
  rownames_to_column(var = "term") %>%
  rename(coefficient = `1`) %>%
  filter(coefficient != 0)

top10coef2 <- coefs2 %>%
  arrange(desc(abs(coefficient))) %>% 
  head(10) %>% 
  mutate(probability = exp(coefficient)/(exp(coefficient)+1)) 

top10coef2 %>%
  kable()

```

### Train

```{r train_glm2}

model_glm2 <- glmnet(train_dtm2, train_labels, alpha = 1, family = "binomial", 
                    lambda = cv_lasso2$lambda.1se) # I choose the largest lambda within 1se from the smallest lambda


```

### Predict

```{r predict_glm2}

#test_dtm_glm <- model.matrix(test_labels ~ test_dtm)[,-1]

predicted_glm2 <- predict(model_glm2, 
                             newx = test_dtm2, 
                             type = "class", # Type "response" gives the fitted probabilities for "binomial"
                             s= cv_lasso2$lambda.1se)


```

### Confusion matrix

```{r confusion_glm2}

test_preds2 <- tibble(review_type = ifelse(test_labels==1,"deceptive","truthful"),
                     prediction = ifelse(predicted_glm2==1, "deceptive", "truthful"),
                     correct = ifelse(review_type == prediction, "1", "0")) %>%
  rowid_to_column("document") 

wrong_reviews <- df[-train_ind,] %>%
  filter(test_preds2[["correct"]]==0)

ggplot(test_preds2, aes(document, prediction, colour = correct)) +
  geom_point(shape = 3, size = 4)

prec2 <- ggplot(test_preds2, aes(prediction, fill = correct)) +
  geom_bar(position = "fill")

recall2 <- ggplot(test_preds2, aes(review_type, fill = correct)) +
  geom_bar(position = "fill")

conf_glm2 <- table(test_labels, predicted_glm2)

conf_glm2 

```


### Performance metrics

```{r performance_glm2}

perf_glm2 <- performance(conf_glm2)

perf_glm2 %>% kable()

```

# Comparison logistic regression models

## Lambda plots

```{r}

tidied_cv <- tidy(cv_lasso) %>% rename(MSE = estimate)
tidied_cv2 <- tidy(cv_lasso2) %>% rename(MSE = estimate)

# plot of MSE as a function of lambda
l_cv<-ggplot(tidied_cv, aes(log(lambda), MSE)) +
  geom_point(colour = "red") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), alpha = .25) +
  geom_vline(xintercept = log(cv_lasso$lambda.min)) +
  geom_vline(xintercept = log(cv_lasso$lambda.1se), lty = 2) +
  ggtitle("Unigram")

l_cv2<-ggplot(tidied_cv2, aes(log(lambda), MSE)) +
  geom_point(colour = "red") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), alpha = .25) +
  geom_vline(xintercept = log(cv_lasso$lambda.min)) +
  geom_vline(xintercept = log(cv_lasso$lambda.1se), lty = 2) +
  ggtitle("Bigram")

ggsave(plot_grid(l_cv, l_cv2), filename = "cv_lambda.png", width = 9, height = 3)

```

## Coefficient tables

```{r}

top10coef %>% kable(format = "latex")

top10coef2 %>% kable(format = "latex") 

```

## Confusion matrix plots

```{r}

ggsave(plot_grid(prec, recall, prec2, recall2), filename = "conf_plots.png", height = 4)

```

## Performance

```{r}

compare_glms <- tibble(metric = perf_mnb[["metric"]],
                       glm = perf_glm[["value"]],
                       glm2 = perf_glm2[["value"]]) %>%
  filter(metric %in% c("precision", "neg predictive value", "recall", "selectivity", "accuracy"))

compare_glms %>% kable(format = "latex")


```


# 3. Classification trees (flexible classifier)


## Train

```{r train_tree}

model_tree <- randomForest(x = train_dtm, y = train_labels, ntree = 1, mtry = ncol(train_dtm))

```

## Predict

```{r predict_tree}

predicted_tree <- predict(model_tree, test_dtm, type = "response")

```

## Confusion matrix 

```{r conf_tree}

test_preds_tree <- tibble(review_type = ifelse(test_labels==1,"deceptive","truthful"),
                     prediction = ifelse(predicted_tree==1, "deceptive", "truthful"),
                     correct = ifelse(review_type == prediction, "1", "0")) %>%
  rowid_to_column("document") 

wrong_reviews <- df[-train_ind,] %>%
  filter(test_preds_tree[["correct"]]==0)

ggplot(test_preds_tree, aes(document, prediction, colour = correct)) +
  geom_point(shape = 3, size = 4)# +
  #facet_grid(~review_type, scales = "free")

ggplot(test_preds_tree, aes(prediction, fill = correct)) +
  geom_bar(position = "fill")

conf_tree <- table(test_labels, predicted_tree)

conf_tree

```

## Performance metrics

```{r performance_tree}

perf_tree <- performance(conf_tree)

perf_tree %>% kable()

```


# 4. Random forests (flexible classifier)

```{r train_random}

# TO DO: cross validate for mtry (caret package)

model_random <- randomForest(x = train_dtm, y = train_labels, ntree = 10, mtry = 5)

```

## Predict

```{r predict_random}

predicted_random <- predict(model_random, test_dtm, type = "response")

```

## Confusion matrix 

```{r conf_random}

test_preds_random <- tibble(review_type = ifelse(test_labels==1,"deceptive","truthful"),
                     prediction = ifelse(predicted_random==1, "deceptive", "truthful"),
                     correct = ifelse(review_type == prediction, "1", "0")) %>%
  rowid_to_column("document") 

wrong_reviews <- df[-train_ind,] %>%
  filter(test_preds_random[["correct"]]==0)

ggplot(test_preds_random, aes(document, prediction, colour = correct)) +
  geom_point(shape = 3, size = 4)# +
  #facet_grid(~review_type, scales = "free")

ggplot(test_preds_random, aes(prediction, fill = correct)) +
  geom_bar(position = "fill")

conf_random <- table(test_labels, predicted_random)

conf_random

```

## Performance metrics

```{r performance_random}

perf_random <- performance(conf_random)

perf_random %>% kable()

```

# Comparison

## Performance metrics

```{r perf_compare}

perf_compare <- tibble(metric = perf_mnb[["metric"]],
                       mnb = perf_mnb[["value"]],
                       glm = perf_glm[["value"]],
                       glm2 = perf_glm2[["value"]],
                       tree = perf_tree[["value"]],
                       random = perf_random[["value"]])

perf_compare %>% kable()


```

## Logistic regression


```{r}

predictions_per_model <- tibble(predictions = c(predicted_mnb, 
                                      as.vector(predicted_glm), 
                                      predicted_tree,
                                      predicted_random),
                      models = c(rep("mnb", 
                                     length(predicted_mnb)), 
                                 rep("logistic regression", 
                                     nrow(predicted_glm)), 
                                 rep("single tree", 
                                     length(predicted_tree)),
                                 rep("random forest",
                                     length(predicted_random))), 
                      ground_truth = c(rep(test_labels, 4)), 
                      correct = ifelse(ground_truth == predictions, 1, 0)) %>%
  mutate(models = factor(models,levels=c("mnb", "logistic regression", "single tree", "random forest")))


glm(correct ~ models, family = "binomial", data = predictions_per_model) %>% 
  tidy() %>%
  kable()


```
